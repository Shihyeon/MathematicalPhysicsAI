{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기말고사\n",
    "## Prob. 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "d\\omega = 0.1\n",
    "$\n",
    "\n",
    "$\n",
    "\\phi = 0.01\n",
    "$\n",
    "\n",
    "$\n",
    "\\Omega \\times V_{0} = 1\n",
    "$\n",
    "\n",
    "$\n",
    "T = 100000\n",
    "$\n",
    "\n",
    "$\n",
    "\\Delta t = 0.001\n",
    "$\n",
    "\n",
    "$\n",
    "U(t=0) = \n",
    "\\begin{pmatrix}\n",
    "1 & 0 \\\\\n",
    "0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = 0\n",
    "phi = 0\n",
    "om_v0 = 1\n",
    "dt = 0.001\n",
    "\n",
    "u0 = torch.tensor([[1, 0], [0, 1]]) * (1. + 0j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "H(t) = -\\frac{1}{2} \\Omega V_{0} s(t) \n",
    "\\begin{pmatrix}\n",
    "0                        & e^{i(\\text{d} \\omega t + \\phi)} \\\\\n",
    "e^{-i(\\text{d} \\omega t + \\phi)} & 0                       \\\\\n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(s_t, T):\n",
    "    t = T*dt\n",
    "    y = -0.5 * om_v0 * s_t * torch.tensor([[0, torch.exp(torch.tensor(+1j * (dw*t + phi)))], \n",
    "                                           [torch.exp(torch.tensor(-1j * (dw*t + phi))), 0]])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fcn = nn.Sequential(nn.Linear(a*8, a*16),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(a*16, a*16),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(a*16, a*8))\n",
    "                                #  nn.ReLU())  # ReLU가 무엇이지??\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.fcn(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)  # lr = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P = \\big|\n",
    "\\begin{pmatrix}\n",
    "0 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "U(t)\n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "\\end{pmatrix}\n",
    "\\big| ^2\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.5261, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\autograd\\__init__.py:251: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21480\\2051388961.py\", line 14, in <module>\n",
      "    ut = ut + dudt*dt\n",
      " (Triggered internally at C:\\b\\abs_2112s1s0to\\croot\\pytorch-select_1700158736573\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:119.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[219], line 23\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog(p))\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# print(\"log:\", loss.size())\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# loss = torch.sum(st**2).requires_grad_(True)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(\"sum:\", loss.size())\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     24\u001b[0m loss_list\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\main\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "loss_list = []\n",
    "list = []\n",
    "# list_str = []\n",
    "\n",
    "ut = u0.detach().clone()\n",
    "\n",
    "for epoch in range(1, 250): # 1000+1\n",
    "    input_t = torch.arange(a*8)*dt\n",
    "    st = model(input_t)\n",
    "    model.train()\n",
    "\n",
    "    for T in range(1, a*8):\n",
    "        dudt = -1j*torch.matmul(H(st[T], T), ut)\n",
    "        ut = ut + dudt*dt\n",
    "    \n",
    "    p = torch.square(torch.matmul(torch.matmul(torch.tensor([[0, 1]]) * (1. + 0j), ut), torch.tensor([[1], [0]])*(1. + 0j)).abs())[0]\n",
    "    # print(\"p:\", p.size(), \"st:\", st.size())\n",
    "    loss = torch.sum(-torch.log(p)).requires_grad_(True)\n",
    "    # print(\"log:\", loss.size())\n",
    "    # loss = torch.sum(st**2).requires_grad_(True)\n",
    "    # print(\"sum:\", loss.size())\n",
    "    # print()\n",
    "    loss.backward()\n",
    "    loss_list.append(loss.item())\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # plt.plot(T, loss[0])\n",
    "    # plt.show()\n",
    "    if epoch % 10 == 1:\n",
    "        print(loss)\n",
    "    #     print(ut)\n",
    "    #     plt.plot(st.detach().numpy(), 'b.')\n",
    "    #     plt.show()\n",
    "        \n",
    "        list.append(loss.item())\n",
    "        # list_str.append(str(loss.item()))\n",
    "        \n",
    "\n",
    "plt.plot(list, 'b.')\n",
    "plt.show()\n",
    "# print(list_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
